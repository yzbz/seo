 # 验证已收录的页面是否有效,并将结果存入数据库
 
 ### 代码需要先安装selenium
 [selenium 安装配置及用法](https://www.jianshu.com/p/f68ae64a3bae)
 
 ### 需要修改内容
 * driver安装目录
 * 数据表创建
 * 数据库配置
 * 写入自己的域名
 * 验证页面内容是否正确

### 建表sql
```

  CREATE TABLE `err_link` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `title` varchar(255) NOT NULL DEFAULT '' COMMENT '标题',
  `url` varchar(500) NOT NULL DEFAULT '' COMMENT '地址',
  `zmw_url` varchar(500) DEFAULT '' COMMENT '网站地址',
  `status` tinyint(1) unsigned DEFAULT '0' COMMENT '0未处理，1链接正常，2失效（空白，404）',
  `addtime` int(11) unsigned NOT NULL DEFAULT '0' COMMENT '添加时间',
  `zmw_id` int(11) unsigned DEFAULT '0',
  `ext` varchar(255) DEFAULT '' COMMENT '文件扩展名',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4

```
代码中增加断网检测，链接是否存在的判断
### 上代码
 
 ```

# -*- coding: utf-8 -*-
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
import os,sys
import time
import pymysql
from bs4 import BeautifulSoup
# 指定driver地址
browser = webdriver.Chrome(executable_path=r"C:\Users\***\Google\Chrome\Application\chromedriver.exe")
browser.maximize_window()

db = pymysql.connect(host='10.59.1.110', user='root', password="root", database='test', port=3306)
cursor = db.cursor()

def ping():
    n = 1
    while n:
        result = os.system(u"ping www.baidu.com -n 3")
        if result == 0 or n == 20:
            break
        time.sleep(5)
        n += 1
    return True

def findExits(url):
    findRes = cursor.execute(
        "select * from err_link where url = %s", url)
    if findRes == 1:
        return True
    else:
        return False

def checkHasCont(url):
    js = "window.open('" + url + "')"
    browser.execute_script(js)
    WebDriverWait(browser, 20, poll_frequency=0.5, ignored_exceptions=None)
    time.sleep(1)
    n = browser.window_handles
    browser.switch_to.window(n[1])
    real_url = browser.current_url
    status = '2'
	# 验证页面内容是否正确
    if browser.page_source.find('验证网址必有文本内容') != -1:
        status = '1'
    browser.close()
    return {'status': status, 'real_url': real_url}


def save_link(url, title, status, real_url):
    if findExits(url) == False:
        cur_time = int(time.time())
        new_id = cursor.execute(
            'insert into err_link (title, url, addtime, status, real_url) values (%s, %s, %s, %s, %s)', (title, url, cur_time, status, real_url)
            )
        if new_id == 0:
            print('insert error')
            sys.exit()
        db.commit()
    else:
        print('url exits')

def getLinks(url, init = False):
    ping()
    browser.get(url)
    if init:
        inputelement = browser.find_element_by_name('wd')
        time.sleep(2)
        inputelement.clear()
		# 写入自己的域名
        inputelement.send_keys("site:baidu.com")
        time.sleep(5)
        submitEle = browser.find_element_by_id('su')
        submitEle.submit()

    time.sleep(4)
    contEle = browser.find_element_by_id('content_left')
    html = contEle.get_attribute('innerHTML')
    soup = BeautifulSoup(html, 'html.parser')

    keyList = soup.select('.t>a')
    for i in range(len(keyList)):
        hasContRes = checkHasCont(keyList[i].attrs['href'])
        save_link(keyList[i].attrs['href'], keyList[i].text, hasContRes['status'], hasContRes['real_url'])
        n = browser.window_handles
        browser.switch_to.window(n[0])
        time.sleep(1)

    time.sleep(5)
    if init:
        nextEle = browser.find_element_by_css_selector('.n')
    else:
        nextEle = browser.find_elements_by_css_selector('.n')[1]

    nextUrl = nextEle.get_attribute("href")
    print(nextUrl)
    getLinks(nextUrl, False)

if __name__ == '__main__':
    try:
        getLinks("https://www.baidu.com/", True)  # 获取收录的链接并存储
    except Exception as e:
        print(e)


```
